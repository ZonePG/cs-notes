# [Nature Machine Intelligence 2021] Estimation of continuous valence and arousal levels from faces in naturalistic conditions

## Abstract

面部情感分析旨在通过使计算机能够更好地理解一个人的情绪状态来创建新的人机交互类型，以提供特别的帮助和互动。由于离散的情绪类别(如愤怒、快乐、悲伤等)不能代表人类每天显示的全部情绪，心理学家通常依赖于维度度量，即效价(valence, 情绪显示的积极程度)和唤醒(arousal, 情绪显示看起来有多平静或兴奋)。然而，虽然从人脸估计这些值对人类来说是很自然的，但基于计算机的系统很难在自然条件下自动估计效价和唤醒是一个悬而未决的问题。此外，这些措施的主观性使得很难获得高质量的数据。在这里，我们引入了一种新的深度神经网络架构，以高精度分析自然条件下的面部影响。该网络集成了人脸对齐，并在单次传递中联合估计分类和连续情绪，使其适用于实时应用。我们在在自然条件下收集的三个具有挑战性的数据集上测试我们的方法，并表明我们的方法优于所有以前的方法。我们还讨论了有关使用此工具的警告，以及在其应用程序中必须考虑的伦理方面。

## Introduction

面部情感分析是一个活跃的研究领域，旨在自动估计一个人的情绪，以提供新的人机交互类型。然而，心理学的最新技术与计算机视觉所做的工作之间通常存在差距。特别是，计算机视觉中的大多数现有工作都集中在一个简单的设置上，即预测实验室条件下显示的情绪的“原型”表达的离散类别。同时，心理学家已经从这些粗略的类别转向，这些粗略的类别不能反映人类在自然、日常情况下每天显示的情感显示范围。此外，虽然我们有兴趣在自然（野外）条件下执行情感显示分析，但大多数现有的工作都集中在受控的实验室条件下。

理论和实践之间的这种不匹配可以用各种因素来解释：
- 在自然条件下收集大量情感数据的困难
- 准确标记大量数据的难度

为了能够很好地推广到看不见的现实生活情况，深度学习需要这两者。这种情况最近开始随着野外收集的数据集的引入而演变，并准确地注释了效价和唤醒（例如，AffecNet、AFEW-VA 和 SEWA）。

几乎所有现有的工作都将任务视为一系列不相交的步骤。首先，在图像上运行人脸检测器以检测它中存在的每个人脸。然后使用人脸检测器预测的边界框裁剪每个人脸以去除背景。在每个人脸上检测面部标志(基准点)，用于将人脸投影到规范坐标系中，以消除人脸框中人脸的平移、旋转和缩放。这些对齐的图像最终被用作机器学习算法的输入，该算法可以解释面部信息，例如情绪。我们的方法使用单个网络在一次传递中估计三种类型的信息：面部地标、离散情绪和连续情绪。这导致影响估计性能有了很大的提高，因为基准点周围的重要面部特征可用于构建注意力机制。它还有助于将这种单步方法实现为实时运行的轻量级模型。

具体来说，在本文做出了以下贡献：
- 我们提出了一种从自然条件下记录的面部显示图像中连续效价和唤醒估计的新方法，大大优于最先进的方法。
- 我们通过提出一种新的深度神经网络架构（图 3）来做到这一点，该架构联合执行面部对齐，并在单次传递中正确预测离散和连续的情感标签。
- 我们简化了从面部图像中识别情感识别的 pipeline，并提供了一系列改进，以在这项任务上取得更好的性能。

## Results

我们设计了一个卷积神经网络，它将一个人的脸的图像作为输入，并估计一个人的对该图像的影响。该估计是根据效价和唤醒水平的连续值完成的。我们表明，我们的方法大大优于所有现有方法，并将其与人工注释者之间的一致性进行比较。

**Baseline comparison.** 为了与目标问题的传统深度学习方法的性能进行比较，我们实现了效价和唤醒估计的基线方法。具体来说，我们直接在数据集和增强数据（即在训练期间应用小的旋转、缩放、平移和图像翻转）。该网络经过训练以最大化一致性相关系数 (CCC)，这是一种始终包含相对误差和相关性元素的度量。该指标已被广泛用于情感估计。

**Our approach.** 与该领域的现有方法相比，我们提出了一种检测面部标志并估计分类和连续情绪的单一网络。该方法利用了人脸点检测的人脸对齐网络 (FAN) 学习到的特征，该网络也与情绪识别任务相关。这增加了完整模型的性能。此外，我们还引入了一系列进一步提高模型性能的步骤。这些步骤包括：分类和连续情绪的联合预测，使网络对数据集中的异常值更加稳健；一种注意力机制驱动关注与情感估计相关的面部区域；学生-教师训练框架，称为知识蒸馏，它平滑网络学习的标签；最后是一个特别定制的损失函数，它导致优化与情感识别相关的指标。

**Results on AffectNet.** 我们在效价估计任务上的 CCC 值上比之前的最新技术 14 高出 17%，在唤醒估计任务上高出 20%（对应于两个任务的 CCC 值增加了 0.11）。在我们的实验中，我们还发现不正确的标签是 AffectNet 数据集中的一大问题。清理这些错误标签的验证和测试集对于能够准确评估模型的性能至关重要。在手动清理的测试和验证集上，CCC 相对于现有技术的改进甚至更大，效价增加了 0.2，唤醒提高了 0.21。有关如何执行手动清理的详细信息在补充信息中提供。

**Results on SEWA.** 我们的方法在所有指标上都优于现有技术。特别是，对于 CCC 值，我们在效价估计任务中获得了 38% 的相对改进，唤醒估计任务获得了 56%（对应于效价的 CCC 值增加 0.18，唤醒提高了 0.22）。

**Results on AFEW-VA.** 我们采用五折独立交叉验证策略来训练我们的网络。由于 AFEW-VA 数据集（大约 30,000 张图像）的规模非常小，使用 ResNet-18 的基线表现出非常差的性能。为了克服这个问题，我们在 AffectNet 数据集上训练我们的 EmoFAN 网络，然后使用五折交叉验证策略在 AFEW-VA 数据集上对其进行微调。生成的网络在所有指标上都大大优于 AFEW-VA 数据集的最新技术。特别是，对于 CCC 值，我们在效价估计任务上实现了 33% 的改进，唤醒估计任务提高了 18%（对应于效价的 CCC 值增加 0.17，唤醒为 0.1）。

**Comparison to human inter-agreement.** 与人类协议间的比较。还进行了与人类协议间的比较。与任何计算机视觉应用一样，用于面部图像情感分析的监督学习方法是使用人类观察者生成的真实标签进行训练的，这些标签被要求根据离散情绪和情感维度（效价和唤醒）手动注释目标面部图像。而在标准的计算机视觉应用(例如，目标检测)中，由于缺乏准确标注所需的上下文知识，人脸图像在显示的情绪方面的注释特别困难(例如，记录的人的文化背景、他们的个性和当前任务)。这导致更高的观察者间变异性，这通常是为每个面部图像数据集报告的。

AffectNet 5 和 SEWA 7 数据集提供了人工协议（表 4）。有趣的是，对于 AffectNet 数据集，如果我们的方法被视为另一个注释器，它在估计效价时与人工注释者的平均一致性至少与任何其他注释器一样好，并且在估计唤醒时，它优于其他注释器的平均一致性，CCC 值平均高出 0.2。这证实了心理学的研究，发现人类在效价方面比估计一个人的面部表情（唤醒）更善于判断面部情感。在 SEWA 数据集上，我们的方法与人类注释者的平均一致性远高于人类注释者自己达成的平均一致性，效价和唤醒的 CCC 值都高出 0.38。

## Applications and ethical considerations

鉴于人们在应对非常广泛的刺激（包括媒体内容、人际对话、社交情况以及人们对他们周围人物和性质的随意观察）方面做出反应，面部情感分析方法有很多潜在的应用。本文中提出的方法是非突兀的，仅使用使用商业可用相机（而不是专门的相机）收集的面部图像，这增加了潜在应用的数量。在纯商业方面，该技术的使用已被证明在市场分析中非常成功，以快速准确地判断大量付费观察者，如或不喜欢某些产品和广告。在应用光谱的纯面向研究方面，面部情感分析技术在各种心理和精神疾病研究中被证明是不可或缺的。例如，从一个人交流的人的脸准确提取情感信息的能力在亲社会性中起着重要作用，这种能力经常被发现在许多精神疾病中发生了变化，其特征是社会功能受损，如精神分裂症、抑郁症和自闭症谱系障碍。这里介绍的那种自动面部情感技术为测量迄今为止抵制测量的情感行为指标开辟了巨大的潜力，因为它们太微妙或由人眼测量。

这里需要注意的是，尽管我们的目标是为自动面部情感分析构建稳健的技术，以用于"人类"应用程序，如上面列出的应用程序，但该技术可能会被滥用。与其他信息一起，该技术可用于在各种通道（例如电话、网络摄像头）上更稳健地识别和跟踪受试者及其行为模式，并使用此信息针对政治或其他目标。反过来，我们认为使用这里介绍的那种自动面部影响技术的每个应用程序都需要正确审计，并且该技术解决方案相对于其他解决方案的风险和优点需要清楚地传达给公共和所有目标应用程序用户。具体来说，我们认为需要足够的审计和解决程序，符合生物识别技术现在制作以及与情感计算技术特有的伦理问题相关的程序。

我们还想强调的是，这里介绍的面部情感分析方法既不能识别人的“最内的情绪”，也不能对各种情绪显示中存在的文化差异敏感。具体来说，这里提出的面部影响技术只能分析某人的脸上描绘的内容，没有其他内容。因此，如果有人选择微笑来掩盖他或她的感觉，该技术将无法正确识别该人的实际情感状态，并将只识别微笑（即“快乐”状态和正价）。此外，情绪显示和情绪显示的感知都不是普遍的；面部表情根据主题和注释者的文化背景以不同的方式显示和解释。因此，用于训练面部影响分析技术的数据可能偏向于训练数据中存在的面部显示（例如，如果大多数受试者是欧洲裔美国人，该方法将偏向于欧洲裔美国人通常显示的面部表情。同样，用于训练面部情感分析技术的数据可能会偏向于面部显示如何根据注释者文化中的情绪来解释。本文介绍的方法的一个版本已经在 AffectNet 数据库上进行了训练。该数据库包含主要是欧洲裔美国人的面部图像，这也是欧洲裔美国人注释者主要注释的。因此，我们在 AffectNet 数据集上训练的方法的版本预计不会以与来自非欧洲裔美国文化背景的数据或由非欧洲裔美国人注释者注释的数据相同的高精度执行。这是最近的 SEWA 数据集采用与文化相关的注释的原因，这意味着注释者与面部情感被注释的人来自同一文化。然而，即使对于我们在 SEWA 数据集上训练的方法版本，仅对我们在 SEWA 数据库中拥有数据的六种文化（即德语、匈牙利语、塞尔维亚语、英国、希腊语和中文）对面部情感表现的文化差异敏感。对于其他文化，如非洲、南亚和南美，该方法对面部情感显示和解释文化特异性差异不敏感。当然，如果这些数据可用，则可以重新训练该方法。

最后，我们想强调的是，上述审计和解决程序需要包括对用于构建自动面部情感技术的训练数据的审计。这里的问题是，这项技术也可以用于看似良性的应用程序，如果技术没有正确训练，这最终可能会严重歧视。示例包括 AI 授权的远程视图分析和/或租赁汽车驾驶员分析，其中可以筛选指示压力或中毒的行为模式。应用的自动面部影响技术需要在适当的人口统计多样化的数据上进行训练，以避免用户的某些部分将被歧视的情况，因为它们的年龄或文化特定的行为模式在所使用的训练数据中的代表性不足。因此，再一次，我们在这里争辩说，充分的审计和补救程序完全符合现在进行生物识别技术的过程，这解决了包括隐私考虑、训练数据、双重使用/误用以及在目标应用中使用情感技术的选择伦理考虑在内的所有问题是必不可少的。

## Emotion estimation from facial imagery

我们的方法在情感识别任务中大大优于最先进的算法。在下文中，我们解释了我们的方法的细节以及我们在我们的方法架构中所做的更改，从而提高了这种性能。

**Datasets.** 在我们的实验中，我们使用了三个数据集视频和图像，这些数据集在自然条件下收集，并由人类专家注释者在效价和唤醒水平方面进行注释。

AffectNet. 大规模面部图像数据集5根据离散和连续的情感标签(效价和唤醒)进行注释。它包含从 Internet 下载的超过一百万张图像以及 66 个面部标志的注释。其中，450, 000 张图像由 12 个人工注释者手动注释。该数据集包含非常大的人口多样性主题。

AFEW-VA. 600 个视频剪辑的数据集 ，跨越 30,000 帧，具有高质量效价和唤醒级别的注释，以及 68 个面部标志，每帧都高度准确注释。

SEWA. 包含超过 2,000 分钟音频和视频数据的大规模多模态数据集，并根据 59 个面部标志和连续效价和唤醒水平丰富注释。它包含来自六种不同文化的 398 个不同的主题，几乎均匀地跨越 20-80 的年龄范围。

**Performance metrics.** 我们评估研究中使用的性能指标是：必须最小化的 RMSE 和必须最大化的 SAGR、PCC 和 CCC。如果 Y 是预测标签，^YI 是真实标签，μY 和 σY 分别表示 Y 的均值和标准差，则这些指标定义如下。我们评估研究中使用的性能指标是：必须最小化的 RMSE 和必须最大化的 SAGR、PCC 和 CCC。如果 $Y$ 是预测标签，$\hat{Y}$ 是真实标签，$\mu_{Y}$ 和 $\sigma_{Y}$ 分别表示 Y 的均值和标准差，则这些指标定义如下。

均方根误差 (RMSE) 评估预测值与目标值的接近程度：

$$
\operatorname{RMSE}(Y, \hat{Y})=\sqrt{\mathbb{E}\left((Y-\hat{Y})^2\right)}
$$

符号一致性 (SAGR) 评估预测值的符号是否与目标值的符号匹配：

$$
\operatorname{SAGR}(Y, \hat{Y})=\frac{1}{n} \sum_{i=1}^n \delta\left(\operatorname{sign}\left(y_i\right), \operatorname{sign}\left(\hat{y}_i\right)\right)
$$

Pearson相关系数 (PCC) 衡量预测值和目标值的相关性为：

$$
\operatorname{PCC}(Y, \hat{Y})=\frac{\mathbb{E}\left(Y-\mu_Y\right)\left(\hat{Y}-\mu_{\hat{Y}}\right)}{\sigma_Y \sigma_{\hat{Y}}}
$$

一致性相关系数 (CCC) 还包含 PCC 值，但惩罚具有不同均值的相关信号。换句话说，如果预测信号具有与目标信号相似的趋势，但值远离目标值（高误差），则会受到低 CCC 的惩罚（尽管 PCC 很高）。

$$
\operatorname{CCC}(Y, \hat{Y})=\frac{2 \sigma_Y \sigma_{\hat{Y}} \operatorname{PCC}(Y, \hat{Y})}{\sigma_Y^2+\sigma_{\hat{Y}}^2+\left(\mu_Y-\mu_{\hat{Y}}\right)^2}
$$

**Combining geometric and appearance information.** 传统的人脸图像情感分析方法由一个 pipeline 组成，该管道首先使用几何信息(以面部标志的形式)对图像进行预处理，然后提取外观特征进行分类或回归，以达到情感识别。这种预处理通过裁剪面部周围的输入图像来消除平移和缩放，这些图像先前基于一组基准点（或地标）对齐。因此，准确的人脸地标检测和人脸对齐是必不可少的，检测到的地标的准确性对算法的性能有重大影响。由于 AffectNet 和 SEWA 等情感数据集中提供的地标最初是使用较旧的、不太准确的方法计算的，我们使用 Bulat 和 Tzimiropoulos 的最先进的面部标志检测器重新计算它们。所有报告的网络都使用这些改进的地标进行训练，并且都显示出它们在两个数据集上的性能有所提高。使用这些更准确的地标获得的改进可以在表 5 的第二行（具有模态 F+R 的 ResNet-18）中看到。

基于上述观察，准确面部标志的检测是必不可少的，我们进一步提出了一个步骤，提出将地标检测和情感识别合并为一个单一的新方法，该方法结合了两者。特别是，我们构建了一个训练网络来检测 landmarks，并且首先结合FAN几层的特征，以及它预测的热图，然后将它们传递到一系列卷积块以预测情感标签。这些特征本质上编码了低级面部特征（例如位于面部部分边界的边缘）和包含某些面部区域位置的高级形态特征（即眼睛、嘴唇）。众所周知，这种几何特征与情绪的面部表情具有很强的相关性，这在约束和自然环境的实践中都得到了经验观察。这使我们能够获得一个单一的网络，该网络联合找到面部标志并估计离散和连续的情绪。由于FAN网络是在包含极端头部姿势和各种面部表情的大型人脸数据集上预先训练的，因此从图像中提取的特征也与情感分析密切相关，并作为情绪预测的弱监督。这种新方法可以显着提高性能，如表 5 的第四行（带有模态 F+R 的 EmoFAN）报告。

**Joint prediction of continuous and discrete affect.** 大多数现有的工作都集中在预测离散情绪或影响维度（“连续情绪”）。相比之下，我们假设模型在必须预测同一面部图像中的离散情绪和连续情绪时不太可能对数据进行错误标记。因此，与之前的工作不同，我们训练网络在注释可用时联合估计离散和连续情绪。因此，我们的网络对数据集中的异常值更加稳健，即针对在离散或连续情感标签方面注释错误的图像。为了合并这一点，我们引入了一种新的损失函数，它是分类损失（离散情感类别；等式（5））和 CCC 损失（等式（6））的交叉熵损失之和，用于连续情感维度估计。

$$
\mathcal{L}_{\text {categories }}(Y, \hat{Y})=\text { Cross entropy }(Y, \hat{Y})=-\sum_{i=1}^n \hat{y}_i \log \left(y_i\right)
$$

$$
\mathcal{L}_{\mathrm{CCC}}(Y, \hat{Y})=1-\frac{\operatorname{CCC}_{\text {valence }}(Y, \hat{Y})+\operatorname{CCC}_{\text {arousal }}(Y, \hat{Y})}{2}
$$

这导致所有指标的进一步改进，如表 5 所示（模态 F+R+C 的第三行和第五行）。

**Attention mechanism.** 并非所有图像中的信息都与情感分类相关。特别是，有强有力的证据表明面部标志周围的区域特别相关，而面部边界上的区域不太相关 6。这是人类中心凹视觉的结果，它将注意力集中在我们看的对象的“中心”部分，而不是它的外围。因此，人类通过进化来学习通过收缩人脸的中心部分来传达视觉信号。传统神经网络没有考虑此信息可以通过注意力机制合并。我们网络中的注意力机制被实现为 FAN 中不同级别的特征与预测的面部标志（热图）的乘积。有关此步骤的附加信息可以在补充信息中找到。这允许网络更好地关注可能对情绪估计很重要的人脸区域，并降低不太有用的区域的重要性。每个热图代表每个地标位置的概率这一事实促进了这一点。因此，它导致指标的改进，如表 5 的第六行所示（具有模态 F+R+C+A 的 EmoFAN）。

**Knowledge distillation.** 知识蒸馏是一种改进网络预测的技术。它分两步工作。首先，教师网络在特定数据集上进行训练。然后第二个网络，称为学生网络，在相同的数据集上进行训练，但使用教师网络预测的标签而不是数据集中给出的标签。这个过程背后的想法是教师网络已经学会了如何平滑数据集中的错误标签。因此，将这些提供给学生网络可以提供更清晰的数据来从中学习。在数学上，由于每个网络的输出是标签上的分布，我们最小化学生 ps 和教师 pt 预测的分布之间的距离，通常使用 KL 散度 18。KL散度的定义是，对于分别对应于教师和学生的预测的两个概率分布 pt 和 ps，如 $\mathrm{KL}\left(\mathrm{p}_{\mathrm{t}} \| \mathrm{p}_{\mathrm{s}}\right)=\sum_{\mathrm{i}=1}^{\mathrm{n}} \mathrm{p}_{\mathrm{t}}(\mathrm{i}) \log \left(\frac{\mathrm{p}_{\mathrm{t}}(\mathrm{i})}{\mathrm{p}_{\mathrm{s}}(\mathrm{i})}\right)$。这导致以下损失项添加到损失函数中：

$\mathcal{L}_{\text {distillation }}\left(p_s, p_t\right)=\mathrm{KL}\left(\mathrm{p}_{\mathrm{t}} \| \mathrm{p}_{\mathrm{s}}\right)=\sum_{\mathrm{i}=1}^{\mathrm{n}} \mathrm{p}_{\mathrm{t}}(\mathrm{i}) \log \left(\frac{\mathrm{p}_{\mathrm{t}}(\mathrm{i})}{\mathrm{p}_{\mathrm{s}}(\mathrm{i})}\right)$

蒸馏的结果在表 5 的第七行中给出（具有模式 F+R+C+A+D 的 EmoFAN），这再次显示出改进。

**Improving the evaluation.** 机器学习的核心是将经验风险最小化为实际风险的良好近似的想法；这规避了评估标签和数据的联合概率分布的需要。因此，一个关键的假设是数据的分布及其标签对于训练集、测试集和验证集是相同的。然而，对于 AffectNet 数据集，我们发现，在验证集和测试集上，很大一部分标签不正确。因此，训练集、验证集和测试集的概率分布可能有很大不同。这种现象被称为标签偏移，严重影响性能，因为最小化验证集上的经验风险不再会导致测试集的良好性能。这就是为什么一般来说，在解释预测时应该考虑此类问题的原因。因此，我们通过手动删除所有不正确的标签来清理测试集和验证集。这为我们提供了一种更好的方法来验证我们模型的超参数并评估其性能，从而大大提高了性能，如表 5 (EmoFAN (clean set)) 的最后一行所示。有关如何清理测试集和验证集的详细信息，请参阅补充信息。

**Loss function.** 对于连续情感预测，我们主要感兴趣的是最大化预测和真实注释之间的相关系数，即 PCC 和 CCC。然而，每个指标都编码了有关目标任务的重要信息（例如，较低的 RMSE 通常会导致更高的 SAGV，因为预测误差较低）。因此，最优预测器应该能够最大化所有这些预测器，同时最小化 RMSE。我们通过将损失函数更改为四个项的总和来编码此信息：离散情绪的分类损失、与最小化 RMSE 相关的损失、最大化 PCC 的损失和最大化 CCC 的损失。此外，回归损失进一步正则化，抖动-抖动正则化系数α，β和γ在训练过程中的每次迭代中随机选择均匀[0;1]。这确保了网络不仅专注于最小化三个回归损失之一。网络最小化的完整损失由下式给出：

$$
\begin{aligned}
& \mathcal{L}(Y, \hat{Y})=\mathcal{L}_{\text {categories }}(Y, \hat{Y})+\frac{\alpha}{\alpha+\beta+\gamma} \mathcal{L}_{\mathrm{MSE}}(Y, \hat{Y}) \\
& +\frac{\beta}{\alpha+\beta+\gamma} \mathcal{L}_{\mathrm{PCC}}(Y, \hat{Y})+\frac{\gamma}{\alpha+\beta+\gamma} \mathcal{L}_{\mathrm{CCC}}(Y, \hat{Y})
\end{aligned}
$$

$$
\begin{gathered}
\mathcal{L}_{\mathrm{MSE}}(Y, \hat{Y})=\operatorname{MSE}_{\text {valence }}(Y, \hat{Y})+\operatorname{MSE}_{\text {arousal }}(Y, \hat{Y}) \\
\mathcal{L}_{\mathrm{PCC}}(Y, \hat{Y})=1-\frac{\operatorname{PCC}_{\text {valence }}(Y, \hat{Y})+\operatorname{PCC}_{\text {arousal }}(Y, \hat{Y})}{2}
\end{gathered}
$$

该损失函数的使用导致网络性能的整体改进。这可以在表 6 中看到，其中我们研究上述定义损失函数的每个部分都导致的改进。特别是，与使用常规 CCC 损失（第一行）训练的基线相比，使用我们的损失函数训练的相同模型具有较低的 RMSE 和更好的 SAGV，而不会降低 CCC 值（带有抖动-摇晃的 EmoFAN）。

**Implementation.** 该实现是使用开源软件完成的，特别是深度学习部分的 PyTorch。我们使用 Adam 优化器 41 训练网络，每 15 个 epoch 学习率降低 10。所有超参数都使用随机网格搜索进行验证。特别是，我们验证了 [0.0, 0.01] 范围内的权重衰减、[0.0001; 0.01] 范围内的学习率和 [0.0; 0.999] 范围内的优化器的参数 beta1 和 beta2。补充信息中提供了更多详细信息和规范。

## Conclusions

在本文中，我们研究了一种深度学习方法，以前所未有的准确性水平在自然条件下进行面部情感分析。我们确认了面部几何信息对这项任务的重要性，信息通常由面部基准地标的位置编码。然后，我们强调了注意力机制对目标情绪估计任务关注图像中最相关的部分的重要性。我们确定了数据注释的一个重要问题，可以通过进一步清理验证集和测试集以及训练深度神经网络同时估计分类和连续情感标签来缓解。然后使用模型蒸馏平滑训练集上的注释问题。我们的方法将所有上述模型合并到一个端到端的可训练模型中，该模型大大优于所有现有的自动面部情感估计工作。

## Data availability

- AFEW-VA: https://ibug.doc.ic.ac.uk/resources/afew-va-database/
- AffectNet: http://mohammadmahoor.com/affectnet/
- SEWA: https://db.sewaproject.eu/

## code availability

预训练的网络、测试代码和清理后的 AffectNet 测试和验证集的注释可在 https://github.com/face-analysis/emonet 获得，基于知识共享署名-NonCommercial-NoDivatives 4.0 International Licence (CC BY-NC-ND)。
